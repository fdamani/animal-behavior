2.26.19
	- run simulation locally make sure we can accurately recover sparsity params.
	- run on full dataset (with decent size learning rate) 
	- run with multiple learning rates.

	- use synthetic data to make sure we can properly recover these new parameters.


2.25.19
- run bootstrap estimator for each file. X
- plots
	- model param estimation: boxplots for each model param across all rats using bootstrap data (wait for run to finish)
	- plot with smoothed reward over time for all rats
		- we have a plot for one. but we might want a few side-by-side panels.
	- one latent trajectory plot (with uncertainty). almost done. different colors for different subplots.
	- PPC: generate y's from data. generate 100 and plot smoothed reward over time. 


- finish latent trajectory plot. X
- bootstrap plot. X
- add these two and smoothed reward plot. X
- other plots
	- ppc
	- 



2.23.19
	agenda:
		- process new rat datasets. X
		- fixed memory leak in inference and estimation. X
		- running experiments on 2 rats (75 trial bins) and 1 rat (10 trial bin)
		- implement bootstrap X
			- all we need to do is write a new log prior batch compl func
				- will not be the same. if we sample a z_t this means we want to 
				compute log prob of z_t conditioned on z_t-1 but this says NOTHING
				about wanting to compute log prob of z_t-1. there is no notion of ordering.
				- what we want is just a matrix of col1=z_t and col2=z_t-1. go find me the log probs.
				- all functions should work for for loop?
		- run on two new datasets and see what the numbers come out to. X
		- implement bootstrap using data from output. X
		- run again on another rat with lower learning rate. X

		- make figures
			- make bootstrap figures
				- boxplots for each model parameter for each rat.
			- plot latent trajectory for one rat.
			- marginal likelihood as we add additional structure to model.
				- 1 alpha vs multiple
				- alpha versus no alpha
				- sparsity versus no sparsity.

2.22.19

	current experiments:
		- show we can get things to converge nicely (loss, params)
			- test different learning rates (1e-2 to 1e-5)
				ANSWER: 1e-4 is the best. although some model params
					did not finish converging so will need to run for
					longer.
			- based on learning rate, try different number of particles
				50, 100, 500, 1000 
				ANSWER
					- 1k gave memory error (do we need to address this?)

		- running marginal likelihood on just random walk (no sparsity, no learning)
		- cant run experiment of random walk + sparsity w/o learning due to cuda memory


given parameters, run on one animal with more reasonable bin size
	to see actual results (e.g. are they interesting-do rats regularize?)

to do:
	- apply above parameters to more datasets
		- process new datasets.
		- run above for all of these different datasets. 
	- write bootstrap code.
		- write this as a function of model output files.
		- read in data.
		- take latents and compute a random sample with replacement
			of size equal to number of time points
		- write down expected complete data log likelihood as a function
		of the random samples
			- likelihood doesn't matter.
			- we just need to sum up log probs of chosen z_t's
				- equivalent to residual bootstrap estimator for AR1 process
			- compute gradients for each pass. save all of the estimates
				in a file.
	- using above info, run an experiment with more data (bin size=10)



	- start with latex template -> identify sections / experiments
	- multiple datasets
	- for one rat, can we properly estimate all model parameters and latent trajectories?
		- pick trajectory with lowest marginal LL. (plot marginal LL)
		- run for x iterations
		- for iter with lowest marginal ll
			- save particles, weights, model paramters, data.
		- 
	- write bootstrap estimation code. 
		- implement residual bootstrap sample residual iid with replacement
		- confirm when taking gradients likelihood does NOT matter.
		- 

2.21.19
	- speed up smc to be parallelized across particles. X 
	- speed up expected complete data log likelihood by vectorizing over particles. X
		- use ideas from the batch code we just wrote to get this to work properly.
	- get all of this working using a GPU. this should make everything very fast using large # of particles. and time-steps (gpus taking awhile to get access to) X

	- move forward with results for 10 sample bins.
	- load all datasets.
	- run until everything converges.
	- evaluation plots.


other optimization:
	- figure out memory leak: rate of mem increase as we go through particle
		filter is very fast when num_obs = 2 but much slower when num_obs=10


misc:
	- prepare precept notes
	- gym 
	- shower
	- teach.



	- get model to work for 1 time point at a time. for simplest model try large number of particles
		and explore many different transition log scales and init log prior scales.
		- if you cant get it to work for 1 time point, find smallest number of time points that it works
		well for.


	- look into particle degenearcy?
	- organize and process many datasets to allow for param estimation 
		across multiple datasets
	- evaluation metrics
		- marginal likelihood comparing models.
			- for each "model param setting" have a dist over marginal likelihoods
				- box and whisker type plot
			- no alpha vs one alpha vs alpha per dimension
				- if alpha per dimension is interesting, think of a way
					of showing this info. maybe something with how its 
					correlated with the latent trajectories of those latents.
				- alpha with a schedule. exponential weighting. 
			- (momentum term)
			- no sparsity versus learned sparsity (sparsity per dim?)
		- for learned model, plot marginal likelihood as a function of sparsity.
			- show that its not constant as sparsity becomes arbitrarily small.
		- hold out 10% of data. how well can you predict it?
		- PPC. given model params, sample 100 trajectories and compute smoothed reward with standard deviations. compare to true.
	- poster
		- latex
		- figures
		- put in placeholders, math, motivation, some references


	- evaluation metrics
		- ppc -> reward plot over time.
		- hold out 10% of trials and report predictive log likelihood
			- how to train with missing y's?
				- at trial t with missing y, how to compute	
				log likelihood?
	- convex combo of l1 and l2 penalty. X
		- (1-alpha)*z + alpha * sgn(z)

	- optional: parallelize across particles for expected
		complete data log likelihood (only nec if we want to use 1k particles
		which we might)

2.20.19
	- feature engineering.
		- choice hist, rw side hist, sensory hist.
	- alpha -> to a vector
	- learn transition log scales.
	- single sparsity parameter.




	- run just with sparsity on simulation. 
		- can we recover params properly?
			- try diff inits. start with large and small.
			- yes it works fine. close interplay between scale
			and sparsity param.
		- any issue with optimiation?
		- no issue

	- run sparsity param on real data. what does it converge to?
	- add learning. what does it convert to?
	- add new features.
	- speed up smc -> parallelize over particles




2.19.19
	- morning agenda:
		- sparsity parameter. test in simulation.
		- alpha vector of parameters.
		- what priors for different params?
			- needed to margianlize variables

	- evening agenda:
		- do not need to marginalize model parameters, just optimization
		- no soft thresholding. 
		- convex combination of l2 and l2 param


2.18.18
	- agenda:
		- finish editing new em inference code. X
			- almost done. figure out why smc code is crashing. compare input to
			old input. b/c old WORKS.
		- check scale of tones. (needs to be in log space so 1 length difference in amplitude)
			- i also normalized.
		- feature engineering. (averaging over last 5 trials)
		- scale per dimension
		- sparsity parameter (want a vector one per dimension)
			- show we can recover parameter in simulation.
		- alpha vector
		- priors on everything to allow for marginalization
			- assess priors based on support from point estimate
		- VI in m-step
		- how to marginalize sparsity parameter?
		- learning rate schedule -> exponential decay. 
		- momentum.
		- compare smc proposals
			- simple ar
			- add structure to smc proposal as prior
			- show marginal likelihood improves when adding structure.
		


	talk with JP
	- update:
		- EM with particle filtering
		- working on simulated and real
		- some features done.
	- talk about
		- sparsity X 
		- VI in m-step: priors over model params
			- beta, alpha, lasso, and transition scale
				- beta gaussian
					- for beta < 1, which OU process.
					- values closer to 0 keeps z to have low variance.
					- different betas allow for different margianl variance. 
				- what is the effective marginal variance over z.
				- sigma^2 and beta will determine 
					- beta and c are pulling it towards 0 and sigma^2
				- gamma on positive reals, or gaussian on log alpha space.
				- log normal prior.
			- inverse gamma on sigma^2.  

		- additional features
			- intercept, x1, x2
				: -1 if choice was left
				: +1 if choice was right
			- side of the true reward
				- -1 if reward was side left
				: win/stay lose switch
				- all i pay attention to is what side i should've gone.
			- side matters, side of true reward.
			- more choice history and more true side history. 
			- or filtered history. 
				- how much reward came from left side.
			- average choice 
			- average reward. 


	- alpha parameter.. and set lasso and beta param ahead of time.
	- nested model comparison. how much better do you do with a single
		alpha versus more? 
		- quantify. 
		- intuitive ordering. 



	- momentum?
	- diminishing learning rate


	- difference between 1 and 2 should be teh same betwee n2 adn 3 




	- exponential family. 

		- model comparison

	- different alphas. 

	- sparsity penalty
		- not traditional formulation
		- why does this keep value above zero?
		- why not constrain the l1 norm of z_t-1 and have a sparsity penalty?

	- inference and estimation
		- EM with particle filtering and MLE for model params
		- VI in m-step: need priors for model parameters
			- right now-> i feed beta through a sigmoid
			- assume model alpha must be >= 0. therefore model log alpha.
	- working on simulated data and real data
	- features:
		- choice history (past 1-3 trials): -1 if choice was 0 (left), +1 if choice was 1 (right)
		- reward side history: -1 if reward side was 0 (left), +1 if reward side was 1 (right) 
		- win/stay lose switch? choice * reward
			- win/stay will be +1, lose/switch will be -1.
		- anything else?
	- should reward be (0,1) or (-1,1)?
	- normalizing stimuli seems wrong - ruins relative scale between the two.
	- goal is interpretable structure in latents: if x is high dimensional with rnn: learn low-d structure and model its evolution.


	- what are different "models" that we want to compare under a marginal likelihood? in some ways this is the "easy" part now that i have inference/estimation code working.
		- policy gradient versus other learning algorithms.
		- epsilon greedy (confirm this is a mixture model?) versus greedy versus thompson sampling. 
			- how does thompson sampling work..what does it mean for the rat to maintain a posterior over its beliefs? this must be different than our posterior over its latent trajectory? 
			- how to write likelihood for these... epsilon * uniform + (1-epsilon)*log reg.

	agenda:
		- feature engineering.
		- proper EM 
			- (with vi in m-step to compute marginal likelihoods)
			- if m-step gives posterior over model parameters, then we must integrate out model params in e-step
				- e-step is now expected log joint under dist over model params.
		- modeling extensions
			- lasso term for dynamics prior
			- fit rnn on input + sparse output.
			- interpret structure in latents.


AGENDA:
	- more model params
		- add sparsity component -> review last meeting notes. see if jonathan can chat tomorrow.
			- why not constrain the l1 norm of z_t-1 and have a sparsity penalty?
		- learning rate per dimension? (simple add - save for later)
	- smc proposal should reflect structure. 
	- write EM class
		- e-step is particle filtering
		- m-step should be its own class with option for MLE or VI (for now just do MLE)
			- takes as an argument the expected complete data log likelihood
	- have initial models with REAL data. 
		- feature engineering. (identify a bunch of different features and relevant theories)
			- non-reward related features - different types of strategies that the rat uses
				- win stay / lose switch
				- left vs right rewards 
					- number of rewards on left vs number of rewards on right
		- batch data into 50 samples per time point
		- given model parameters can we do posterior inference
	- what are different "models" that we want to compare under a marginal likelihood? in some ways this is the "easy" part now that i have inference/estimation code working.
		- policy gradient versus other learning algorithms.
		- epsilon greedy (confirm this is a mixture model?) versus greedy versus thompson sampling. 
			- how does thompson sampling work..what does it mean for the rat to maintain a posterior over its beliefs? this must be different than our posterior over its latent trajectory? 
			- how to write likelihood for these... epsilon * uniform + (1-epsilon)*log reg.
	- semi-parametric model.
		- rnn with architecture creating sparse representation. need a sparse prior on weights of logistic regresssion.

2.16.18
	- reward for individual time pointsX  
	- transition to multiple by summing or averaging gradient. X
	- we have wroking estimation of beta and alpha with reward. X
	- we need complete data log likelihood to be FASTER. X
		- vectorize computation -> dont loop over time. compute likelihood and prior in a vectorized fashion using matrix multiplication X
			- this will involve adding a a time dimension at the beginning of everything. X

2.15.18

	-finish implementing rat reward for one time point at a time
		- once we have this working
		- transitioning to multiple should involve just summing and taking mean if needed
		- show we can accurately estimate alpha
	- show we can learn all of these parameters with em
	- work on real data
		- feature engineering: 


	- increase dimensionality of X and Z to 4. X this is working.
	- try fitting to nick's simulation. X
	- once this works, add dynamics and do parameter estimation in M-step.
		- add alpha * policy gradient update
		- add sparsity dynamics described in the meeting yesterday
	- transition to real data 
		- feature engineering
			- start with two tones and a bias
				- fit model and params
			- add different kinds of history
				- refit
				

	- introduce parameters to transition dynamics -> can we recover them with EM?
		- beta term in front of z.
		- alpha and learning component.
	- fit this model to data.
		- feature engineering
			- start with sound1, sound2, intercept with simple ar dynamics.
				- a) show sound1 goes up and sound2 goes down (or vice versa) and bias goes to 0.
			- add past reward, etc. (look at nicks, jiyuns paper ask JP about other features that
				would be interesting to cosyne community)




		set up we liked: (0.0, 0.1) for everything. 50 obs samples, 250 num particles, t = 100


2.14.18

	- slower timescale (one latent per ten steps) is EQUIVALENT to slower mixing
		- so this means prior needs to be slower
			- it doesn't matter how the data is generated...but at training time,
				model parameters need to be SMALLER than the simulation that generated the data.


	- finish implementing actual model. 
		- increase x dimension to 5. make sure the rest of the code works.
		- if we sample multiple realizations from the latent and use all of the extra samples
			for the likelihood, can we converge to the right dynamics. assess error as the number
			of observations goes down.  X
				- 

		- add more options to dynamics prior.
			- learning component. (policy gradient)
			- alpha learning rate
	- parameter estimation 
		- can you recover alpha?
		- complete data log likelihood under gaussian with empirical mean, scale X

	- try out with nicks simulation code -> how does performance compare to ours?
	- try out model with multiple realizations of observation? single latent trajectory for each
		time point generates multiple observations. 

	- look at nicks simulation code -> try using it to generate data. how does it compare to ours? 
	- 


2.13.18
	evening agenda:
		- SMC em.
			- we have particle filtering working. check to see if it is computing the correct variance?
			- m-step: stochastic optimization of complete data likelihood under non-parametric posterior. 


	- talk with rpa
		- start with smc
		- do smc with EM
			- e-step: non-parametric approx of the posterior conditioned on model parameters
			- m-step: gradients of expected complete data log likelihood with respect to model params
				under approximate posterior.
		- specify structure of transition dynamic as the proposal for the smc.
		- start with the simple thing (just smc) and when it breaks we deal with it then.
		- don't jump to fivo/smc. questionable whether it works. they dont backprop gradients through resampling!
			(which is what the paper is about)




	- confirmed forward pass of particle filter is working and giving correct answer.

	- to do
		- look at marginal ll computation from before vs now. how is it different? how does it compare to papers?
		- slowly shift smc sampler to be vectorized and use the ideas from smcopt. where does it break?

		
		- check how to compute log marginal ll
		- confirm approach in smcOPT gives same value as smc approach for log marginal lh.
		- look at papers and figure out a good simple test.
		- review approach compare to paper.


2.12.18
	- continue implementing SMC
		- question:
			- after computing incremental weight alpha:
				1
					- normalize alpha
					- multiply by normalized weights w
					- normalize w*alpha?
				2
					- compute log unnormalized alphas
					- multiply by log weights w
					- normalize w*alpha
				- for now, we resample at every step
			- after finishing sampling last time point, do we resample to collapse?
				- in other words, do we want num_particles unique trajectories?
					- see if you can find answer to this online.
			- if we dont resample, then final weights are just incremental weights:
				alpha = p(x_t, z_t) / q_t(z_t)


	- to do:
		- implement SMC (we know its working - when noise goes to 0 -> approximation becomes perfect) X
		- implement log marginal likelihood X
		- confirm log marginal likelihood is CORRECT. X
			- print kalman filter log marginal computation X
		- implement VSMC
			- set up simple proposal variational family AR-1 model. with single
				mean shift as variational param to optimize. 
					- extension includes one lambda per time point



		- implement VSMC with with learned "q params" which are AR-1 model.
			- learn one init scale, one transition scale as variational params.
			- learn mean for each 
		- compare to true LDS






2.11.18
	- implement SMC (write out steps on paper) - quick heuristic for
		proposal q(z_t | z_{t-1}, noise)
	- apply it to LDS
	- implement iwae bound with a neural network
	-


	- status:
		- working importance sampler
	- to do
		- correctly implement variance of importance sampler
		- have a working SMC sampler on LDS.
			- extending IS to SMC class
			- its just sequential IS
			- resample particles if ESS of weights falls below thresh
		- extend IS to IWAE using elbo + parametric proposal as a q
		- extend SMC to VSMC
		- for each extension log results on benchmarks
		- will want better benchmark than linear system.

2.8.18

	- refactored main to take use arg parse
	- refactored models and inference
		- models owns its own model parameters and sets requires_grad accordingly
		- latent variables are independent and initialized in main (might want to switch this to inference class)


	agenda:
		- implement smc for LDS gaussian model (and compare to exact inference)
			- importance sampling
			- effective sample size
			- renormalize weights
		- implement iwae, smc, fivo (hard)
		- implement dynamics model (easy)



	detailed agenda:
		- implement linear regression X
		- can swap between map and vi X 
		- have a plot comparing VI, IS on linear regression (show they converge to same solution)
		- implement exact inference for lin reg and lds
		- add IWAE estimator and compute on linear regression
		- replot all of the above estimators for a LDS
		- add SMC.



	*****computing IS estimate incorrectly. 


2.7.18
AGENDA:

	- variational cov matrix initialization
	- does the above work for the log reg lds model?
	- does this work for lds with a structured vi model
		- different parameterizations
			- full covariance
			- diagonal plus low rank
			- tridiagonal covariance (e.g. ar-1 model)
		- do these families help with better model param
			estimation? does it help with var mean estimation?


	- LDS
		- joint optimization of variational parameters and model parameters works well
		- adding a band of var scale params below diagonal improves performance and gets us closer to true model params

	- logistic reg LDS
		- joint optimization of model params and var params does not work (they both converge at solutions far away from true)
		- if i set model params to true values, optimize var params, then set var params to converged values and optimize model
			params i get the right solution. 
		- one solution: coordinate procedure, but kind of annoying. what are best practices for joint optimization?



	convo with rpa:
		- turn down noise in LDS -> does bbvi converge to right solution?
		- posterior predictive checks (compare to true data)
		- compare to exact inference
		- implement smc 


2.6.18
	- so far, MAP and Mean-field posteroir inference work well for LDS and Log reg lds (less well for second)
	-for LDS, conditioned on correct latents, can compute map estimate of transition and obs scale perfectly
	- for MFVI, we can correctly estimate model parameters when conditioned on previously estimated variational posterior which was estimated by conditioning on true model parameters
		- we can properly estimate these model parameters regardless of where we initialize
			the transition and obs scale (3 orders of magnitude bigger and smaller)
	- MFVI: joint estimation of variational and model parameters leads to very similar variational params with transition_scale=0.13 and obs_scale=0.08 where true=0.1.
		- mean(var_scale) = 0.0608 versus 0.057 for "true" variational posterior.
		- does this explain discrepancy?
		- results are robust to initialization and converges to same model params.


2.4.18

	convo with rpa
		- problem with mean field fully factorized approach is it underestimated joint posterior correlations
		- need a richer posterior approximation 
		- spoke to ryan about amortization vs richness of posterior
		- conclusion: parameterizing with rnn doesn't help bc its a deterministic computation - we need randomness 
		- try AR-1, some covariance structure, hmm

	agenda:
		- implement new model class with nonlinear transition matrix
			- try a simpler nonlinear transition model.
				- can we recover correct model parameters?
		- improve vi
			- compute posterior mean, variances and model parameters for lds and log reg

	- add python special key that lets you break out of optimization but not cancel program



2.3.18
	- wrote model class
	- wrote lds, log_reg_lds
	- wrote VI inference class
	- implemented above models with gradient descent for map estimation
