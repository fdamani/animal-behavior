2.12.18
	- continue implementing SMC
		- question:
			- after computing incremental weight alpha:
				1
					- normalize alpha
					- multiply by normalized weights w
					- normalize w*alpha?
				2
					- compute log unnormalized alphas
					- multiply by log weights w
					- normalize w*alpha
				- for now, we resample at every step
			- after finishing sampling last time point, do we resample to collapse?
				- in other words, do we want num_particles unique trajectories?
					- see if you can find answer to this online.

	- to do:
		- implement SMC X



2.11.18
	- implement SMC (write out steps on paper) - quick heuristic for
		proposal q(z_t | z_{t-1}, noise)
	- apply it to LDS
	- implement iwae bound with a neural network
	-


	- status:
		- working importance sampler
	- to do
		- correctly implement variance of importance sampler
		- have a working SMC sampler on LDS.
			- extending IS to SMC class
			- its just sequential IS
			- resample particles if ESS of weights falls below thresh
		- extend IS to IWAE using elbo + parametric proposal as a q
		- extend SMC to VSMC
		- for each extension log results on benchmarks
		- will want better benchmark than linear system.

2.8.18

	- refactored main to take use arg parse
	- refactored models and inference
		- models owns its own model parameters and sets requires_grad accordingly
		- latent variables are independent and initialized in main (might want to switch this to inference class)


	agenda:
		- implement smc for LDS gaussian model (and compare to exact inference)
			- importance sampling
			- effective sample size
			- renormalize weights
		- implement iwae, smc, fivo (hard)
		- implement dynamics model (easy)



	detailed agenda:
		- implement linear regression X
		- can swap between map and vi X 
		- have a plot comparing VI, IS on linear regression (show they converge to same solution)
		- implement exact inference for lin reg and lds
		- add IWAE estimator and compute on linear regression
		- replot all of the above estimators for a LDS
		- add SMC.



	*****computing IS estimate incorrectly. 


2.7.18
AGENDA:

	- variational cov matrix initialization
	- does the above work for the log reg lds model?
	- does this work for lds with a structured vi model
		- different parameterizations
			- full covariance
			- diagonal plus low rank
			- tridiagonal covariance (e.g. ar-1 model)
		- do these families help with better model param
			estimation? does it help with var mean estimation?


	- LDS
		- joint optimization of variational parameters and model parameters works well
		- adding a band of var scale params below diagonal improves performance and gets us closer to true model params

	- logistic reg LDS
		- joint optimization of model params and var params does not work (they both converge at solutions far away from true)
		- if i set model params to true values, optimize var params, then set var params to converged values and optimize model
			params i get the right solution. 
		- one solution: coordinate procedure, but kind of annoying. what are best practices for joint optimization?



	convo with rpa:
		- turn down noise in LDS -> does bbvi converge to right solution?
		- posterior predictive checks (compare to true data)
		- compare to exact inference
		- implement smc 


2.6.18
	- so far, MAP and Mean-field posteroir inference work well for LDS and Log reg lds (less well for second)
	-for LDS, conditioned on correct latents, can compute map estimate of transition and obs scale perfectly
	- for MFVI, we can correctly estimate model parameters when conditioned on previously estimated variational posterior which was estimated by conditioning on true model parameters
		- we can properly estimate these model parameters regardless of where we initialize
			the transition and obs scale (3 orders of magnitude bigger and smaller)
	- MFVI: joint estimation of variational and model parameters leads to very similar variational params with transition_scale=0.13 and obs_scale=0.08 where true=0.1.
		- mean(var_scale) = 0.0608 versus 0.057 for "true" variational posterior.
		- does this explain discrepancy?
		- results are robust to initialization and converges to same model params.


2.4.18

	convo with rpa
		- problem with mean field fully factorized approach is it underestimated joint posterior correlations
		- need a richer posterior approximation 
		- spoke to ryan about amortization vs richness of posterior
		- conclusion: parameterizing with rnn doesn't help bc its a deterministic computation - we need randomness 
		- try AR-1, some covariance structure, hmm

	agenda:
		- implement new model class with nonlinear transition matrix
			- try a simpler nonlinear transition model.
				- can we recover correct model parameters?
		- improve vi
			- compute posterior mean, variances and model parameters for lds and log reg

	- add python special key that lets you break out of optimization but not cancel program



2.3.18
	- wrote model class
	- wrote lds, log_reg_lds
	- wrote VI inference class
	- implemented above models with gradient descent for map estimation
